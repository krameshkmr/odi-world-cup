---
title: "A Predictive Model for ODI World Cup Matches"
author: "Krishna Kumar, Srihari Srinivasan"
date: "APR 30, 2023"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(tinytex)
library(cricketdata)
library(leaps)
library(car)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(out.width = "60%")
```

## Introduction

Given that this is a One Day International (ODI) World Cup year, our analysis of batting and bowling performance metrics on the margin of victory, `mov`, is focused on prior World Cup matches. With knowledge of how the ODI format has changed since the introduction of the shorter, Twenty20 (T20) format, particularly the 2007 T20 World Cup, we have limited our data to the last five ODI World Cups (2003, 2007, 2011, 2015, and 2019). After wrangling six batting metrics and four bowling metrics from 4774 individual player performances, we are looking to build a predictive model and gain insight as to which metrics, if any, have an effect on `mov`. In doing so, while this may be beyond the scope of this project, we hope to ultimately use our model to predict the outcome of the 2023 ODI World Cup.

## Exploratory Analysis

```{r, echo=FALSE}
odi <- read.csv("~/Downloads/odi.csv")
odi$date <- as.Date(odi$date)
odi$date <- format(odi$date, "%Y")
odi <- rename(odi, year = date)
```

### Response Variable

`mov` is the difference between the target set by the team batting first and the total that the chasing team achieved. For example, in a 2003 match between England and Pakistan, England scored 246 runs, setting 247 as the target for Pakistan to chase. They, however, were bowled out for 134, resulting in England winning by 112 runs. Therefore, the `mov` for this match is calculated as $winner\_margin / target = 112 / 247 = 0.453$.\

The figure below is a boxplot of `mov` values.\

```{r, echo=FALSE}
odi %>% ggplot(aes(x = mov)) + 
        geom_boxplot(fill = "skyblue", color = "black") + 
        labs(x = "Margin of Victory") +
        theme_bw()

summary(odi$mov)
```

From the figure and summary, we can see that `mov` is approximately normally distributed on 7.1%. The single outlier is a 2011 match between Kenya and New Zealand in which Kenya lost by 84.0%. While large `mov` values can be attributed to a blowout, the extreme values probably occurred due to a wide skill gap between two teams. New Zealand, for example, is an established cricketing nation with a strong, experienced team compared to Kenya.

### Predictor Variables

Batting metrics:\
Given that top-order batsmen (players 1, 2, and 3) generally play a different role to that of middle-order batsmen (players 4, 5, 6, and 7), these metrics have been separated by batting position.\
1. `to_runs_pct` - percentage of total runs scored by the top-order\
2. `mo_runs_pct` - percentage of total runs scored by the middle-order\
3. `to_mins_pct` - percentage of total time spent in crease by the top-order\
4. `mo_mins_pct` - percentage of total time spent in crease by the middle-order\
5. `to_bf_pct` - percentage of total balls faced by the top-order\
6. `mo_bf_pct` - percentage of total balls faced by the middle-order\
7. `pct_4s` - percentage of total runs that are 4s\
8. `pct_6s` - percentage of total runs that are 6s\
9. `to_sr` - average strike rate of the top-order\
10. `mo_sr` - average strike rate of the middle-order\

Bowling metrics:\
1. `bowlers_used` - total number of bowlers used\
2. `pct_mdns` - percentage of total overs that are maidens (an over in which no runs are scored)\
3. `wkts` - total number of wickets taken\
4. `econ` - average number of runs conceded per over bowled\

The figures below are scatterplots of the pairwise relationship between predictor variables.\

```{r, echo=FALSE, out.width = "100%"}
odi[, 6:20] %>% select(c("to_runs_pct", "mo_runs_pct", "to_mins_pct", "mo_mins_pct", "to_bf_pct", "mo_bf_pct", "mov")) %>% pairs()
```

```{r, echo=FALSE, out.width = "100%"}
odi[, 6:20] %>% select(c("pct_4s", "pct_6s", "to_sr", "mo_sr", "mov")) %>% pairs()
```

```{r, echo=FALSE, out.width = "100%"}
odi[, 16:20] %>% pairs()
```

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("~/Documents/GitHub/odi-world-cup/cor.png")
```

From the figures and correlation matrix, we can see that `runs_pct`, `mins_pct`, and `bf_pct` have high collinearity. This exists because the impact of a top-order batsman is not entirely independent of a middle-order batsman. Although the correlation of certain variables exceeds $\pm 0.80$, we decided to let the backward step-wise process filter out and select the metrics with the greatest effect on `mov`.

## Model Development

Our objective at each step of the backward step-wise selection process is to minimize *RMSE* and maximize *adjusted R-squared*. First, we split our data into training and test sets. The training set consists of World Cup matches from 2003 to 2015, whereas the test set is composed solely of 2019 data.\

```{r, echo=FALSE}
train_set <- odi %>% filter(year %in% c(2003, 2007, 2011, 2015))
train_set <- train_set[, 6:20]

test_set <- odi %>% filter(year == 2019)
test_set <- test_set[, 6:20]

model_1 <- lm(mov~., data=train_set)
summary(model_1)
```

After training our model using all 14 predictor variables, we observe an *adjusted R-squared* value of 0.881, meaning we are able to explain 88.1% of the variation in `mov` with all 14 variables.

Using this model, we predict `mov` based on the test set and calculate the *RMSE* value displayed below.

```{r, echo=FALSE}
predictions <- predict(model_1, test_set)
rmse <- sqrt(mean((test_set$mov - predictions)^2))
rmse
```

Normalizing this by dividing *RMSE* by the range of `mov` from the test set results in the following value.

```{r, echo=FALSE}
rmse / (max(test_set$mov) - min(test_set$mov))
```

We further verify these relationships with the use of added-variable plots.\

```{r, echo=FALSE, out.width = "100%"}
avPlots(model_1, layout = c(3,5))
```

Backward step-wise selection removes each of these 14 variables from the model, one at a time, and checks for improvement in the performance metric *RMSE*. Using a loop, we remove one variable at a time from the training set. We remove the $i$-th column using `train_set[,-i]`. This allows us to iterate through columns 1 to 14.

We want to find the lowest *RMSE* value from the table below and remove the predictor variable associated with it. The lowest value appears to be 0.142 which is the 10th row and corresponds to the `mo_sr` variable. We will then remove this variable from the training set and repeat this process.

```{r, echo=FALSE}
#create empty vector to hold accuracies
rmse_1 <- data.frame(vals=rep(NA,14))

for (i in 1:14) {

  #remove i-th variable
  train_min1 <- train_set[,-i]
  
  #fit model minus one variable
  model_min1 <- lm(mov~.,data=train_min1)
  
  #classify testing observations
  test_min1 <- test_set %>%
    mutate(preds=predict(model_min1,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_1$vals[i] <- sqrt(mean((test_set$mov - test_min1$preds)^2))
}
rmse_1
```

The table below shows all of the *RMSE* values after removing `mo_sr`. The lowest value appears to be 0.138 which is the 10th row and corresponds to the `bowlers_used` variable. We will then remove this variable from the training set and repeat this process.

```{r, echo=FALSE}
#permanently remove one variable
train_min1 <- train_set[,-10]

#create empty vector to hold accuracies
rmse_2 <- data.frame(vals=rep(NA,13))

for (i in 1:13) {

  #remove i-th variable
  train_min2 <- train_min1[,-i]
  
  #fit model minus one variable
  model_min2 <- lm(mov~.,data=train_min2)
  
  #classify testing observations
  test_min2 <- test_set %>%
    mutate(preds=predict(model_min2,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_2$vals[i] <- sqrt(mean((test_set$mov - test_min2$preds)^2))
}
rmse_2
```

The table below shows all of the *RMSE* values after removing `bowlers_used`. The lowest value appears to be 0.136 which is the 5th row and corresponds to the `to_bf_pct` variable. We will then remove this variable from the training set and repeat this process.

```{r, echo=FALSE}
#permanently remove one variable
train_min2 <- train_min1[,-10]

#create empty vector to hold accuracies
rmse_3 <- data.frame(vals=rep(NA,12))

for (i in 1:12) {

  #remove i-th variable
  train_min3 <- train_min2[,-i]
  
  #fit model minus one variable
  model_min3 <- lm(mov~.,data=train_min3)
  
  #classify testing observations
  test_min3 <- test_set %>%
    mutate(preds=predict(model_min3,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_3$vals[i] <- sqrt(mean((test_set$mov - test_min3$preds)^2))
}
rmse_3
```

The table below shows all of the *RMSE* values after removing `to_bf_pct`. The lowest value appears to be 0.1339 which is the 7th row and corresponds to the `pct_6s` variable. We will then remove this variable from the training set and repeat this process.

```{r, echo=FALSE}
#permanently remove one variable
train_min3 <- train_min2[,-5]

#create empty vector to hold accuracies
rmse_4 <- data.frame(vals=rep(NA,11))

for (i in 1:11) {

  #remove i-th variable
  train_min4 <- train_min3[,-i]
  
  #fit model minus one variable
  model_min4 <- lm(mov~.,data=train_min4)
  
  #classify testing observations
  test_min4 <- test_set %>%
    mutate(preds=predict(model_min4,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_4$vals[i] <- sqrt(mean((test_set$mov - test_min4$preds)^2))
}
rmse_4
```

The table below shows all of the *RMSE* values after removing `pct_6s`. The lowest value appears to be 0.12886 which is the 4th row and corresponds to the `mo_mins_pct` variable. We will then remove this variable from the training set and repeat this process.

```{r, echo=FALSE}
#permanently remove one variable
train_min4 <- train_min3[,-7]

#create empty vector to hold accuracies
rmse_5 <- data.frame(vals=rep(NA,10))

for (i in 1:10) {

  #remove i-th variable
  train_min5 <- train_min4[,-i]
  
  #fit model minus one variable
  model_min5 <- lm(mov~.,data=train_min5)
  
  #classify testing observations
  test_min5 <- test_set %>%
    mutate(preds=predict(model_min5,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_5$vals[i] <- sqrt(mean((test_set$mov - test_min5$preds)^2))
}
rmse_5
```

The table below shows all of the *RMSE* values after removing `mo_mins_pct`. The lowest value appears to be 0.1289 which is the 2nd row and corresponds to the `mo_runs_pct` variable. However, since this value is slightly higher than the previous lowest *RMSE* value, we can stop the process and create our final model.

```{r, echo=FALSE}
#permanently remove one variable
train_min5 <- train_min4[,-4]

#create empty vector to hold accuracies
rmse_6 <- data.frame(vals=rep(NA,9))

for (i in 1:9) {

  #remove i-th variable
  train_min6 <- train_min5[,-i]
  
  #fit model minus one variable
  model_min6 <- lm(mov~.,data=train_min6)
  
  #classify testing observations
  test_min6 <- test_set %>%
    mutate(preds=predict(model_min6,newdata=test_set))
  
  #compute and save overall accuracy
  rmse_6$vals[i] <- sqrt(mean((test_set$mov - test_min6$preds)^2))
}
rmse_6
```

Our final model, after taking out `mo_sr`, `bowlers_used`, `to_bf_pct`, `pct_6s`, and `mo_mins_pct`, has an *adjusted R-Squared* of 0.857 and an *RMSE* value of 0.129. Therefore, we are able to account for 85.7% of the variation in `mov`.

```{r, echo=FALSE}
model_odi <- lm(mov~., data=train_min5)
summary(model_odi)
```

Normalizing the *RMSE* results in the below value.

```{r, echo=FALSE}
predictions <- predict(model_odi, test_set)
rmse <- sqrt(mean((test_set$mov - predictions)^2))
rmse / (max(odi$mov) - min(odi$mov))
```

Verifying the relationships with the updated added-variable plots.\

```{r, echo=FALSE, out.width = "100%"}
avPlots(model_odi, layout = c(3,3))
```

## Model Analysis

## Conclusion
